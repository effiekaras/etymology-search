# GOALS
## **Leading Question**
We want to analyze the evolution of words to answer the question, “How have the words we use evolved and changed over time?” We plan on using English words and directly related words from other languages according to the dataset as our nodes, which will be connected to other words if they are derived from or etymologically related to the other words in an unweighted, acyclic, directed graph. We aim to display the intriguing developments of common English words as a teaching/observable database, especially with graphical output. One of our hopes is to produce a general search tool, where we can input English words that are included in the database and produce a hierarchical list of predecessors.

## **Dataset Acquisition and Processing** 
We chose to use https://github.com/droher/etymology-db dataset. It was created using semi-structured text parsing, in order to produce a “structured, comprehensive, and multilingual etymology dataset created by parsing Wiktionary's etymology sections.” From this repository, we have been able to download and store the data as a csv file. As of right now, we plan on processing the data such that we look at all English words and include them in our graph. We would then recursively add all the words related to English words and their related words in our graphs such that it goes back to the “original” root word. Additionally, we want to remove unnecessary columns, although not concrete, we are considering the following: position, related_term_id, related_lang, group_tag, parent_tag, parent_position. We are currently planning on storing vertices as words in a hash map. And, we want to store directed edges as the EdgeList: {parent term, derived term, reltype} = {predecessor, successor, label} such that the edges point out of the root words and into the words derived from them, and the edge label is the relationship type (potentially stored as an ENUM). After we have the data stored as an EdgeList, we want to build an Adjacency List. This is better for us than a Matrix because our directed graph will take up a a lot of space. We’ve already begun to notice an error in the dataset and have created a solution. One error we noticed in the text parsing is the conversion from a Wiktionary code to a language, back to a Wiktionary code. Because they were incorrectly stored, some related terms have the wrong related language. However, our relationship is still being preserved, and since we aren’t displaying the related language, it’s unimportant that it may be incorrect. Otherwise, if we have missing pieces in our dataset we will have to consider the following. If the word does not have a related term because it is not derived from anything, then it is a root, and has no predecessors. If it is simply missing one of these two key components, then we will remove this row from our dataset and not consider them in our graph, because they lack the necessary information to answer our question in order to be relevant data.

## **Graph Algorithms** 
* DFS - The traversal we want to implement is a Depth-First Search. We would need to create and store a visited data structure, and store our nodes in a stack. We will have to consider how we are storing separate connected components in separate stacks, and we are also considering implementing DFS class and implementing an iterator as well. We expect overall runtime of O(m+n). The input of the traversal will be the constructed graph implementation and the output will be the list of nodes in the graph.

* Layered graph drawing - We believe a layered graph drawing would be perfect for displaying the hierarchical relationship between our terms. The inputs we expect this drawing to need are simply the graph nodes and directed edges. To make sure everything is level, we will need to consider height also, so we need to decide if we want to call height(), or store the heights after running the program once, since we have static data. The output for this graph should depict multiple connected components with the grandfather/root terms at the top level, and derived terms at each height below it. We expect the algorithm may require O(mn) time. If we have extra time, we will attempt a near-linear time implementation, since more complex implementations allow this.

* Iterative Deepening DFS - This method will take in the word that we are searching for and output the node that contains that word. Then, we can use that node to find the word’s predecessors, answering our leading question. We expect a time complexity of O(b<sup>k</sup>) where b is the number of children in each node (branching factor) and k is the depth.

## **Timeline** 
* Week 1 (11/8-11/14) - Data acquisition, Data processing, Graph construction
* Week 2 (11/15-11/21) - Begin test suite, DFS traversal, Layered Graph Drawing
* Week 3 (11/22-11/28 Thanksgiving Break) - Break, Continue writing test suite
* **MID-POINT CHECK**: Our goal is to have the DFS traversal and substantial progress on LGD.
* Week 4 (11/29-12/5) - Work on Iterative Deepening, Finish test suite
* Week 5 (12/5-12/12) - Debugging/Room for delays, Complete presentation/deliverables